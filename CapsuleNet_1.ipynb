{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "file_extension": ".py",
      "pygments_lexer": "ipython3",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "version": "3.6.3",
      "name": "python"
    },
    "colab": {
      "name": "Copy_of_CapsuleNet_on_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyanendar/ms_project/blob/main/CapsuleNet_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7f4c4917-22fb-4b10-879e-51c600e6c3af",
        "_uuid": "9b086e5ec535ad75eada3ca72bf5e6534251074f",
        "id": "LQnu9movo8xz"
      },
      "source": [
        "# Overview\n",
        "\n",
        "The new model CapsuleNet proposed by Sara Sabour (and Geoffry Hinton) claims to deliver state of the art results on [MNIST](https://arxiv.org/abs/1710.09829). The kernel aims to create and train the model using the Kaggle Dataset and then make a submission to see where it actually ends up. Given the constraint of using a Kaggle Kernel means it can't be trained as long as we would like or with GPU's but IMHO if a model can't be reasonably well trained in an hour on a 28x28 dataset, that model probably won't be too useful in the immediate future.\n",
        "\n",
        "## Implementation Details\n",
        "\n",
        "* Keras implementation of CapsNet in Hinton's paper Dynamic Routing Between Capsules.\n",
        "* Code adapted from https://github.com/XifengGuo/CapsNet-Keras/blob/master/capsulenet.py\n",
        "*  Author: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n",
        "*     The current version maybe only works for TensorFlow backend. Actually it will be straightforward to re-write to TF code.\n",
        "*     Adopting to other backends should be easy, but I have not tested this. \n",
        "\n",
        "Result:\n",
        "    Validation accuracy > 99.5% after 20 epochs. Still under-fitting.\n",
        "    About 110 seconds per epoch on a single GTX1070 GPU card\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "e30e2e10-a909-485d-be9d-dc6f592911a7",
        "_uuid": "c7e569699c6d067cd9fdf9c77299775e399b2ef3",
        "id": "qWnuqFe2o8x2",
        "outputId": "15cdb39b-58b1-4c3c-e5bf-13208eeae02b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#from keras.preprocessing.image import ImageDataGenerator\n",
        "#from keras import callbacks\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import callbacks\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import initializers, layers\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9776fd57-44e0-4211-a7a5-c7e647a10704",
        "_uuid": "f4b5499a472b312d5c5f0274ad429567aced6841",
        "id": "xYHeLw1do8x5"
      },
      "source": [
        "# Capsule Layers \n",
        "Here is the implementation of the necessary layers for the CapsuleNet. These are not optimized yet and can be made significantly more performant. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "167d39ca-ee32-4eec-a83b-86194252b14f",
        "_uuid": "90c180a9a8c20e3fb8a93c3eb42588927cfcd6b6",
        "id": "dQti96GTo8x5"
      },
      "source": [
        "\n",
        "\n",
        "class Length(layers.Layer):\n",
        "    \"\"\"\n",
        "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
        "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
        "    inputs: shape=[None, num_vectors, dim_vector]\n",
        "    output: shape=[None, num_vectors]\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return tf.sqrt(tf.reduce_sum(tf.square(inputs), -1) + K.epsilon())\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Length, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "class Mask(layers.Layer):\n",
        "    \"\"\"\n",
        "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
        "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
        "    masked Tensor.\n",
        "    For example:\n",
        "        ```\n",
        "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
        "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
        "        out = Mask()(x)  # out.shape=[8, 6]\n",
        "        # or\n",
        "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
        "        ```\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
        "            assert len(inputs) == 2\n",
        "            inputs, mask = inputs\n",
        "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
        "            # compute lengths of capsules\n",
        "            x = tf.sqrt(tf.reduce_sum(tf.square(inputs), -1))\n",
        "            # generate the mask which is a one-hot code.\n",
        "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
        "            mask = tf.one_hot(indices=tf.argmax(x, 1), depth=x.shape[1])\n",
        "\n",
        "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
        "        # mask.shape=[None, num_capsule]\n",
        "        # masked.shape=[None, num_capsule * dim_capsule]\n",
        "        masked = K.batch_flatten(inputs * tf.expand_dims(mask, -1))\n",
        "        return masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if type(input_shape[0]) is tuple:  # true label provided\n",
        "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
        "        else:  # no true label provided\n",
        "            return tuple([None, input_shape[1] * input_shape[2]])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Mask, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param vectors: some vectors to be squashed, N-dim tensor\n",
        "    :param axis: the axis to squash\n",
        "    :return: a Tensor with same shape as input vectors\n",
        "    \"\"\"\n",
        "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + K.epsilon())\n",
        "    return scale * vectors\n",
        "\n",
        "\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the\n",
        "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
        "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
        "    :param num_capsule: number of capsules in this layer\n",
        "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
        "    :param routings: number of iterations for the routing algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "\n",
        "        # Transform matrix, from each input capsule to each output capsule, there's a unique weight as in Dense layer.\n",
        "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
        "                                        self.dim_capsule, self.input_dim_capsule],\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='W')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
        "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule, 1]\n",
        "        inputs_expand = tf.expand_dims(tf.expand_dims(inputs, 1), -1)\n",
        "\n",
        "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
        "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule, 1]\n",
        "        inputs_tiled = tf.tile(inputs_expand, [1, self.num_capsule, 1, 1, 1])\n",
        "\n",
        "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
        "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
        "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule, 1]\n",
        "        # Regard the first two dimensions as `batch` dimension, then\n",
        "        # matmul(W, x): [..., dim_capsule, input_dim_capsule] x [..., input_dim_capsule, 1] -> [..., dim_capsule, 1].\n",
        "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
        "        inputs_hat = tf.squeeze(tf.map_fn(lambda x: tf.matmul(self.W, x), elems=inputs_tiled))\n",
        "\n",
        "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        # b.shape = [None, self.num_capsule, 1, self.input_num_capsule].\n",
        "        b = tf.zeros(shape=[inputs.shape[0], self.num_capsule, 1, self.input_num_capsule])\n",
        "\n",
        "        assert self.routings > 0, 'The routings should be > 0.'\n",
        "        for i in range(self.routings):\n",
        "            # c.shape=[batch_size, num_capsule, 1, input_num_capsule]\n",
        "            c = tf.nn.softmax(b, axis=1)\n",
        "\n",
        "            # c.shape = [batch_size, num_capsule, 1, input_num_capsule]\n",
        "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "            # The first two dimensions as `batch` dimension,\n",
        "            # then matmal: [..., 1, input_num_capsule] x [..., input_num_capsule, dim_capsule] -> [..., 1, dim_capsule].\n",
        "            # outputs.shape=[None, num_capsule, 1, dim_capsule]\n",
        "            outputs = squash(tf.matmul(c, inputs_hat))  # [None, 10, 1, 16]\n",
        "\n",
        "            if i < self.routings - 1:\n",
        "                # outputs.shape =  [None, num_capsule, 1, dim_capsule]\n",
        "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "                # The first two dimensions as `batch` dimension, then\n",
        "                # matmal:[..., 1, dim_capsule] x [..., input_num_capsule, dim_capsule]^T -> [..., 1, input_num_capsule].\n",
        "                # b.shape=[batch_size, num_capsule, 1, input_num_capsule]\n",
        "                b += tf.matmul(outputs, inputs_hat, transpose_b=True)\n",
        "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
        "\n",
        "        return tf.squeeze(outputs)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'num_capsule': self.num_capsule,\n",
        "            'dim_capsule': self.dim_capsule,\n",
        "            'routings': self.routings\n",
        "        }\n",
        "        base_config = super(CapsuleLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
        "    \"\"\"\n",
        "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
        "    :param dim_capsule: the dim of the output vector of capsule\n",
        "    :param n_channels: the number of types of capsules\n",
        "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
        "    \"\"\"\n",
        "    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
        "                           name='primarycap_conv2d')(inputs)\n",
        "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
        "    return layers.Lambda(squash, name='primarycap_squash')(outputs)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7cd17730-22b6-4ac3-a612-31f18902fa78",
        "_uuid": "61c38c7ee701bb3ee2190263cf907fcdbe40dca2",
        "id": "UDdQcGoko8x8"
      },
      "source": [
        "# Build the Model\n",
        "Here we use the layers to build up the model. The model is a bit different from a standard $X\\rightarrow y$  model, it is $(X,y)\\rightarrow (y,X)$ meaning it attempts to predict the class from the image, and then at the same time, using the same capsule reconstruct the image from the class. The approach appears very cGAN-like where the task of reconstructing better helps the model 'understand' the image data better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "bc101123-d53c-4c2e-a187-101c434885da",
        "_uuid": "2497453eb1895f624ad84617dd98c230f5640304",
        "id": "HCMIciV0o8x9"
      },
      "source": [
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class CapsuleNet:\n",
        "  @staticmethod\n",
        "  def build(input_shape,number_of_class,routing_count,batch_size,\n",
        "            n_channels = 32,dim_capsule=16,\n",
        "            kernels=[9],filters=[256],strides=[1]):\n",
        "    \n",
        "   \n",
        "   A Capsule Network on MNIST.\n",
        "   :param input_shape: data shape, 3d, [width, height, channels]\n",
        "   :param n_class: number of classes\n",
        "   :param routings: number of routing iterations\n",
        "   :param batch_size: size of batch\n",
        "   :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
        "           `eval_model` can also be used for training.\n",
        "   \n",
        "\n",
        "   input = layers.Input(shape=input_shape, batch_size=batch_size)  \n",
        "   for index in range(kernels):\n",
        "      if index == 0 :\n",
        "         conv = layers.Conv2D(filters=filters[index], \n",
        "                             kernel_size=kernels[index], \n",
        "                             strides=strides[index], \n",
        "                             padding='valid', \n",
        "                             activation='relu', \n",
        "                             name='conv'+str(index))(input)\n",
        "      else:\n",
        "         conv = layers.Conv2D(filters=filters[index], \n",
        "                             kernel_size=kernels[index], \n",
        "                             strides=strides[index], \n",
        "                             padding='valid', \n",
        "                             activation='relu', \n",
        "                             name='conv'+str(index))(conv)\n",
        "  \n",
        "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
        "    :param dim_capsule: the dim of the output vector of capsule\n",
        "    :param n_channels: the number of types of capsules\n",
        "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
        "  \n",
        "   conv_1 = layers.Conv2D(filters=dim_capsule*n_channels, \n",
        "                             kernel_size=kernels[-1], \n",
        "                             strides=strides[-1], \n",
        "                             padding='valid', \n",
        "                             activation='relu', \n",
        "                             name='primary_capsule_conv')(conv)\n",
        "    \n",
        "   conv_2 = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(conv_1)\n",
        "\n",
        "   primary_capsule = layers.lambda(squash,name = 'primary_capsule')(conv_2)\n",
        "\n",
        "    # Capsule layer. Routing algorithm works here.\n",
        "   digitcaps = CapsuleLayer(num_capsule=number_of_class, \n",
        "                             dim_capsule=dim_capsule, \n",
        "                             routings=routing_count, \n",
        "                             name='digitcaps')(primary_capsule)\n",
        "\n",
        "    # Auxiliary layer to replace each capsule with its length.\n",
        "    # Just to match the true label's shape.\n",
        "    \n",
        "   out_caps = Length(name='capsnet')(digitcaps)\n",
        "\n",
        "    # Decoder network.\n",
        "   y = layers.Input(shape=(number_of_class,))\n",
        "   masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
        "   masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n",
        "\n",
        "    # Shared Decoder model in training and prediction\n",
        "   decoder = models.Sequential(name='decoder')\n",
        "   decoder.add(layers.Dense(512, activation='relu', input_dim=16 * number_of_class))\n",
        "   decoder.add(layers.Dense(1024, activation='relu'))\n",
        "   decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
        "   decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
        "\n",
        "    # Models for training and evaluation (prediction)\n",
        "   train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
        "   eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
        "  \n",
        "   return train_model, eval_model                            \n",
        "\"\"\"\n",
        "\n",
        "def CapsNet(input_shape, n_class, routings, batch_size):\n",
        "    \"\"\"\n",
        "    A Capsule Network on MNIST.\n",
        "    :param input_shape: data shape, 3d, [width, height, channels]\n",
        "    :param n_class: number of classes\n",
        "    :param routings: number of routing iterations\n",
        "    :param batch_size: size of batch\n",
        "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
        "            `eval_model` can also be used for training.\n",
        "    \"\"\"\n",
        "    x = layers.Input(shape=input_shape, batch_size=batch_size)\n",
        "\n",
        "    # Layer 1: Just a conventional Conv2D layer\n",
        "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
        "\n",
        "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
        "    primarycaps = PrimaryCap(conv1, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
        "\n",
        "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
        "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings, name='digitcaps')(primarycaps)\n",
        "\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    out_caps = Length(name='capsnet')(digitcaps)\n",
        "\n",
        "    # Decoder network.\n",
        "    y = layers.Input(shape=(n_class,))\n",
        "    masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
        "    masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n",
        "\n",
        "    # Shared Decoder model in training and prediction\n",
        "    decoder = models.Sequential(name='decoder')\n",
        "    decoder.add(layers.Dense(512, activation='relu', input_dim=16 * n_class))\n",
        "    decoder.add(layers.Dense(1024, activation='relu'))\n",
        "    decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
        "    decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
        "\n",
        "    # Models for training and evaluation (prediction)\n",
        "    train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
        "    eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
        "\n",
        "   \n",
        "    return train_model, eval_model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c6d84e5a-c33c-40c8-89c3-aba3454f7025",
        "_uuid": "9f27c6b0623ebffb6c8a24579f9dd4e321d6b1c2",
        "id": "LS09Ic7qo8x_"
      },
      "source": [
        "def margin_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
        "    :param y_true: [None, n_classes]\n",
        "    :param y_pred: [None, num_capsule]\n",
        "    :return: a scalar loss value.\n",
        "    \"\"\"\n",
        "    # return tf.reduce_mean(tf.square(y_pred))\n",
        "    L = y_true * tf.square(tf.maximum(0., 0.9 - y_pred)) + \\\n",
        "        0.5 * (1 - y_true) * tf.square(tf.maximum(0., y_pred - 0.1))\n",
        "\n",
        "    return tf.reduce_mean(tf.reduce_sum(L, 1))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7d75c4e0-ffca-45fc-8acc-620fe2825f82",
        "_uuid": "3e810fabad89045b7f4b51fe8540164f90e7698c",
        "id": "LjrtUDsVo8yE"
      },
      "source": [
        "# Load MNIST Data\n",
        "Here we load and reformat the Kaggle contest data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JBMdPcyZrla"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "169b7f78-12c7-4fed-886e-60024fe59339",
        "_uuid": "02b7db879a533e7bfb3116522bebf3867b23498c",
        "id": "0zSuXAxxo8yQ"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import csv\n",
        "import math\n",
        "import pandas\n",
        "\n",
        "def combine_images(generated_images, height=None, width=None):\n",
        "    num = generated_images.shape[0]\n",
        "    if width is None and height is None:\n",
        "        width = int(math.sqrt(num))\n",
        "        height = int(math.ceil(float(num)/width))\n",
        "    elif width is not None and height is None:  # height not given\n",
        "        height = int(math.ceil(float(num)/width))\n",
        "    elif height is not None and width is None:  # width not given\n",
        "        width = int(math.ceil(float(num)/height))\n",
        "\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image\n",
        "\n",
        "def plot_log(filename, show=True):\n",
        "\n",
        "    data = pandas.read_csv(filename)\n",
        "\n",
        "    fig = plt.figure(figsize=(4,6))\n",
        "    fig.subplots_adjust(top=0.95, bottom=0.05, right=0.95)\n",
        "    fig.add_subplot(211)\n",
        "    for key in data.keys():\n",
        "        if key.find('loss') >= 0 and not key.find('val') >= 0:  # training loss\n",
        "            plt.plot(data['epoch'].values, data[key].values, label=key)\n",
        "    plt.legend()\n",
        "    plt.title('Training loss')\n",
        "\n",
        "    fig.add_subplot(212)\n",
        "    for key in data.keys():\n",
        "        if key.find('acc') >= 0:  # acc\n",
        "            plt.plot(data['epoch'].values, data[key].values, label=key)\n",
        "    plt.legend()\n",
        "    plt.title('Training and validation accuracy')\n",
        "\n",
        "    # fig.savefig('result/log.png')\n",
        "    if show:\n",
        "        plt.show()    \n",
        "\n",
        "def test(model, data, args):\n",
        "    x_test, y_test = data\n",
        "    y_pred, _ = model.predict(x_test)#, batch_size=args.batch_size)\n",
        "    print('-' * 30 + 'Begin: test' + '-' * 30)\n",
        "    print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1)) / y_test.shape[0])\n",
        "\n",
        "    #img = combine_images(np.concatenate([x_test[:50], x_recon[:50]]))\n",
        "    #image = img * 255\n",
        "    #Image.fromarray(image.astype(np.uint8)).save(args.save_dir + \"/real_and_recon.png\")\n",
        "    #print()\n",
        "    #print('Reconstructed images are saved to %s/real_and_recon.png' % args.save_dir)\n",
        "    #print('-' * 30 + 'End: test' + '-' * 30)\n",
        "    #plt.imshow(plt.imread(args.save_dir + \"/real_and_recon.png\"))\n",
        "    #plt.show()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot**"
      ],
      "metadata": {
        "id": "kEL29LHiYu9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model_performance(history):\n",
        "  plt.style.use(\"ggplot\")\n",
        "  plt.figure(figsize=(7,6))\n",
        "                    \n",
        "  epoch_ran = len(history.history[\"loss\"])\n",
        "  plt.plot(np.arange(0, epoch_ran), \\\n",
        "           history.history[\"loss\"], label=\"train_loss\")\n",
        "  plt.plot(np.arange(0, epoch_ran), \\\n",
        "           history.history[\"val_loss\"], label=\"val_loss\")\n",
        "  plt.plot(np.arange(0, epoch_ran), \\\n",
        "           history.history[\"accuracy\"], label=\"train_acc\")\n",
        "  plt.plot(np.arange(0, epoch_ran), \\\n",
        "           history.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "  plt.title(\"Training Loss and Accuracy\")\n",
        "  plt.xlabel(\"Epoch #\")\n",
        "  plt.ylabel(\"Loss/Accuracy\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  #Print best score \n",
        "  print(\"\\nModel Performance Summary:\\n\")"
      ],
      "metadata": {
        "id": "Exz-8L7fV9HZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "e698ab37-4e74-43e6-b35b-a0507449d916",
        "_uuid": "c0374dabdf452026e3f231ffe689b5dd9f99288b",
        "id": "TZJyVmido8yK"
      },
      "source": [
        "def train(model, data, args):\n",
        "    \"\"\"\n",
        "    Training a CapsuleNet\n",
        "    :param model: the CapsuleNet model\n",
        "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
        "    :param args: arguments\n",
        "    :return: The trained model\n",
        "    \"\"\"\n",
        "    # unpacking the data\n",
        "    (x_train, y_train), (x_test, y_test) = data\n",
        "\n",
        "    # callbacks\n",
        "    log = callbacks.CSVLogger(args.save_dir + '/log.csv')\n",
        "    checkpoint = callbacks.ModelCheckpoint(args.save_dir + '/weights.h5', monitor='val_capsnet_accuracy',\n",
        "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
        "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: args.lr * (args.lr_decay ** epoch))\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(optimizer=optimizers.Adam(lr=args.lr),\n",
        "                  loss=[margin_loss, 'mse'],\n",
        "                  loss_weights=[1., args.lam_recon],run_eagerly=True,\n",
        "                  metrics={'capsnet': 'accuracy'})\n",
        "\n",
        "\n",
        "    # Begin: Training with data augmentation ---------------------------------------------------------------------#\n",
        "    def train_generator(x, y, batch_size, shift_fraction=0.):\n",
        "        train_datagen = ImageDataGenerator()  # shift up to 2 pixel for MNIST\n",
        "        generator = train_datagen.flow(x, y, batch_size=batch_size)\n",
        "        while 1:\n",
        "            x_batch, y_batch = generator.next()\n",
        "            yield (x_batch, y_batch), (y_batch, x_batch)\n",
        "\n",
        "    # Training with data augmentation. If shift_fraction=0., no augmentation.\n",
        "    history = model.fit(train_generator(x_train, y_train, args.batch_size, args.shift_fraction),\n",
        "              steps_per_epoch=int(y_train.shape[0] / args.batch_size),\n",
        "              epochs=args.epochs,\n",
        "              validation_data=((x_test, y_test), (y_test, x_test)), batch_size=args.batch_size,\n",
        "              callbacks=[log, checkpoint, lr_decay])\n",
        "\n",
        "    \n",
        "    index = history.history[\"val_capsnet_accuracy\"].index(max(history.history[\"val_capsnet_accuracy\"]))\n",
        "    best_val_accuracy = history.history[\"val_capsnet_accuracy\"][index]    \n",
        "    print(f\"Best Validation Accuracy:{best_val_accuracy}\") \n",
        "    \n",
        "    #plot_model_performance(history)\n",
        "\n",
        "    #model.save_weights(args.save_dir + '/trained_model.h5')\n",
        "    #print('Trained model saved to \\'%s/trained_model.h5\\'' % args.save_dir)\n",
        "\n",
        "    #plot_log(args.save_dir + '/log.csv', show=True)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_train_test(data,args):\n",
        "  set_seed()\n",
        "  (x_train,y_train),(x_test,y_test) = data\n",
        "  model, eval_model = CapsNet(input_shape=x_train.shape[1:],\n",
        "                                                  n_class=len(np.unique(np.argmax(y_train, 1))),\n",
        "                                                  routings=args.routings,\n",
        "                                                  batch_size=args.batch_size)\n",
        "  model.summary()\n",
        "  trained_model = train(model=model, data=((x_train, y_train), (x_test, y_test)), args=args)\n",
        "  trained_model.load_weights(f'./result/weights.h5')\n",
        "  y_pred,_= trained_model.predict((x_test,y_test), batch_size=x_test.shape[0])\n",
        "  print('-' * 30 + 'Begin: test' + '-' * 30)\n",
        "  print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1)) / y_test.shape[0])  \n",
        " \n",
        "  result_pred = np.argmax(y_pred, axis=1) \n",
        "  result_actual = np.argmax(y_test, axis=1) \n",
        "  precision = precision_score(result_actual, result_pred)\n",
        "  recall = recall_score(result_actual, result_pred)\n",
        "  f1 = f1_score(result_actual, result_pred)\n",
        "  tn, fp, fn, tp = confusion_matrix(result_actual, result_pred).ravel()\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(result_actual, result_pred)\n",
        "  auc = metrics.auc(fpr, tpr)\n",
        "  \n",
        "  print(f\"Precision:{precision}\")\n",
        "  print(f\"recall:{recall}\")\n",
        "  print(f\"f1:{f1}\")\n",
        "  print(f\"AUC:{auc}\")\n",
        "  print(f\"Sensitivity:{tp/(tp+fn)}\")\n",
        "  print(f\"specificity:{tn/(tn+fp)}\")\n"
      ],
      "metadata": {
        "id": "DaOxiCiiW5fQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EKbfeEhiYkr",
        "outputId": "bf5b128f-766c-44f8-a716-bc6ecc0988e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"Capsule Network on MNIST.\")\n",
        "parser.add_argument('--epochs', default=15, type=int)\n",
        "parser.add_argument('--batch_size', default=32, type=int)\n",
        "parser.add_argument('--lr', default=0.0001, type=float,\n",
        "                        help=\"Initial learning rate\")\n",
        "parser.add_argument('--lr_decay', default=1, type=float,\n",
        "                        help=\"The value multiplied by lr at each epoch. Set a larger value for larger epochs\")\n",
        "parser.add_argument('--lam_recon', default=0.392, type=float,\n",
        "                        help=\"The coefficient for the loss of decoder\")\n",
        "parser.add_argument('-r', '--routings', default=1, type=int,\n",
        "                        help=\"Number of iterations used in routing algorithm. should > 0\")\n",
        "parser.add_argument('--shift_fraction', default=0.1, type=float,\n",
        "                        help=\"Fraction of pixels to shift at most in each direction.\")\n",
        "parser.add_argument('--debug', action='store_true',\n",
        "                        help=\"Save weights by TensorBoard\")\n",
        "parser.add_argument('--save_dir', default='./result')\n",
        "parser.add_argument('-t', '--testing', action='store_true',\n",
        "                        help=\"Test the trained model on testing dataset\")\n",
        "parser.add_argument('--digit', default=5, type=int,\n",
        "                        help=\"Digit to manipulate\")\n",
        "parser.add_argument('-w', '--weights', default=None,\n",
        "                        help=\"The path of the saved weights. Should be specified when testing\")\n",
        "args, unknown = parser.parse_known_args()\n",
        "print(args)\n",
        "\n",
        "if not os.path.exists(args.save_dir):\n",
        "    os.makedirs(args.save_dir)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=32, debug=False, digit=5, epochs=15, lam_recon=0.392, lr=0.0001, lr_decay=1, routings=1, save_dir='./result', shift_fraction=0.1, testing=False, weights=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "9dbcd67c-8f99-4d8b-8cf7-480d8dc069a4",
        "_uuid": "526436cc40013621251285812ba95725d4a6d749",
        "id": "7UfmktlEo8yF"
      },
      "source": [
        "def load_mnist():\n",
        "    \n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
        "    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
        "    y_train = to_categorical(y_train.astype('float32'))\n",
        "    y_test = to_categorical(y_test.astype('float32'))\n",
        "\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load RIM-ONE Dataset**"
      ],
      "metadata": {
        "id": "pyoKrohAY3Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVhC0mWybBFf",
        "outputId": "1aa7a171-6897-4378-930a-0affa7625018"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RIM_ONE_DL_DIR = '/content/gdrive/My Drive/MSC_Project/Dataset/RIM-ONE_DL_images/partitioned_randomly'\n",
        "RIM_ONE_R2_DIR = '/content/gdrive/My Drive/MSC_Project/Dataset/RIMONE-db-r2'\n",
        "ACRIMA_DIR = '/content/gdrive/My Drive/MSC_Project/Dataset/ACRIMA'"
      ],
      "metadata": {
        "id": "99C4kIlhbJdD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HEIGHT= 64\n",
        "WIDTH = 64\n",
        "BATCH_SIZE = 32\n",
        "CLASS_COUNT = 2\n",
        "NUM_EPOCHS_A = 25\n",
        "NUM_EPOCHS_B = 40"
      ],
      "metadata": {
        "id": "kv4_0aYMbNC_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize image to 256*256\n",
        "# Convert to numpy array\n",
        "def process_image(data_set_dir, label_dict, width,height):\n",
        "    \n",
        "    x = [] # will store images as arrays\n",
        "    y = [] # store labels\n",
        "    # list folders in directory\n",
        "    directories = os.listdir(data_set_dir)\n",
        "     \n",
        "    # for each folder (train and validation) \n",
        "    for label in directories:\n",
        "        \n",
        "        # add class label to label dictionary\n",
        "        if label not in label_dict:\n",
        "            label_dict[label] = len(label_dict)\n",
        "        \n",
        "        # create full path for image directory \n",
        "        source_images = os.path.join(data_set_dir, label)\n",
        "        images = os.listdir(source_images)\n",
        "        # for each image in directory, \n",
        "        for image in images:\n",
        "            #folder have .txt files which needs to be ignored\n",
        "            if '.txt'not in image:\n",
        "                # read the image from file, resize and add to a list\n",
        "                full_size_image = cv2.imread(os.path.join(source_images, image))\n",
        "                #gray_image = cv2.cvtColor(full_size_image, cv2.COLOR_BGR2GRAY)\n",
        "                \n",
        "                #append the image to x\n",
        "                x.append(cv2.resize(full_size_image, (width,height), \n",
        "                                                            interpolation=cv2.INTER_CUBIC))\n",
        "                # add the class label to y\n",
        "                y.append(label)\n",
        "\n",
        "    data = np.array(x, dtype=\"float\") / 255.0                \n",
        "    label = np.array(y)\n",
        "    \n",
        "    return data,label"
      ],
      "metadata": {
        "id": "RwbtEuh3bQXF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rim_one_dl():\n",
        "  class_labels = {}\n",
        "  training_images,training_labels = process_image(RIM_ONE_DL_DIR+\"/training_set\",class_labels,WIDTH,HEIGHT)\n",
        "  training_label = (pd.Series(training_labels).map(class_labels)).values\n",
        "  training_label = to_categorical(training_label.astype('float32'))\n",
        "  training_images = training_images.reshape(-1, WIDTH, WIDTH, 3).astype('float32')\n",
        "\n",
        "  test_images,test_labels = process_image(RIM_ONE_DL_DIR+\"/test_set\",class_labels,WIDTH,HEIGHT)\n",
        "  test_label = (pd.Series(test_labels).map(class_labels)).values\n",
        "  test_label = to_categorical(test_label.astype('float32'))\n",
        "  test_images = test_images.reshape(-1, WIDTH, WIDTH, 3).astype('float32')\n",
        "\n",
        "  return (training_images,training_label),(test_images,test_label)"
      ],
      "metadata": {
        "id": "WYYvphuBabld"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rim_one_db_r2():\n",
        "  class_labels = {}\n",
        "  training_images,training_labels = process_image(RIM_ONE_R2_DIR,class_labels,WIDTH,HEIGHT)\n",
        "  training_label = (pd.Series(training_labels).map(class_labels)).values\n",
        "  training_label = to_categorical(training_label.astype('float32'))\n",
        "  training_images = training_images.reshape(-1, WIDTH, WIDTH, 3).astype('float32')\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(training_images, training_label, test_size=0.2, random_state=100)\n",
        "\n",
        "  return (X_train,y_train),(X_test,y_test)"
      ],
      "metadata": {
        "id": "aI44L23fgLGf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_acrima_dataset(test_train_ratio = 0.2):\n",
        "  class_labels = {}\n",
        "  training_images,training_labels = process_image(ACRIMA_DIR,class_labels,WIDTH,HEIGHT)\n",
        "  training_label = (pd.Series(training_labels).map(class_labels)).values\n",
        "  training_label = to_categorical(training_label.astype('float32'))\n",
        "  training_images = training_images.reshape(-1, WIDTH, WIDTH, 3).astype('float32')\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(training_images, training_label, test_size=test_train_ratio, random_state=100)\n",
        "\n",
        "  return (X_train,y_train),(X_test,y_test)"
      ],
      "metadata": {
        "id": "iU4ArbgNZBKZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed = 100):\n",
        "  tf.random.set_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)"
      ],
      "metadata": {
        "id": "oeRFsDfUlXC0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3yrXFySoqCM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = get_rim_one_db_r2()\n",
        "get_model_train_test(data,args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GECoDk0NqCFG",
        "outputId": "86ddbb5e-1b04-421a-f8b9-12d7a0891f26"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(32, 64, 64, 3)]    0           []                               \n",
            "                                                                                                  \n",
            " conv1 (Conv2D)                 (32, 56, 56, 256)    62464       ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " primarycap_conv2d (Conv2D)     (32, 24, 24, 256)    5308672     ['conv1[0][0]']                  \n",
            "                                                                                                  \n",
            " primarycap_reshape (Reshape)   (32, 18432, 8)       0           ['primarycap_conv2d[0][0]']      \n",
            "                                                                                                  \n",
            " primarycap_squash (Lambda)     (32, 18432, 8)       0           ['primarycap_reshape[0][0]']     \n",
            "                                                                                                  \n",
            " digitcaps (CapsuleLayer)       (32, 2, 16)          4718592     ['primarycap_squash[0][0]']      \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 2)]          0           []                               \n",
            "                                                                                                  \n",
            " mask_4 (Mask)                  (32, 32)             0           ['digitcaps[0][0]',              \n",
            "                                                                  'input_6[0][0]']                \n",
            "                                                                                                  \n",
            " capsnet (Length)               (32, 2)              0           ['digitcaps[0][0]']              \n",
            "                                                                                                  \n",
            " decoder (Sequential)           (None, 64, 64, 3)    13137408    ['mask_4[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,227,136\n",
            "Trainable params: 23,227,136\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/11 [==============================] - ETA: 0s - loss: 0.4695 - capsnet_loss: 0.4425 - decoder_loss: 0.0688 - capsnet_accuracy: 0.5057\n",
            "Epoch 1: val_capsnet_accuracy improved from -inf to 0.45652, saving model to ./result/weights.h5\n",
            "11/11 [==============================] - 8s 681ms/step - loss: 0.4695 - capsnet_loss: 0.4425 - decoder_loss: 0.0688 - capsnet_accuracy: 0.5057 - val_loss: 0.4215 - val_capsnet_loss: 0.3957 - val_decoder_loss: 0.0659 - val_capsnet_accuracy: 0.4565 - lr: 1.0000e-04\n",
            "Epoch 2/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4132 - capsnet_loss: 0.3865 - decoder_loss: 0.0680 - capsnet_accuracy: 0.4048\n",
            "Epoch 2: val_capsnet_accuracy did not improve from 0.45652\n",
            "11/11 [==============================] - 7s 663ms/step - loss: 0.4132 - capsnet_loss: 0.3865 - decoder_loss: 0.0680 - capsnet_accuracy: 0.4048 - val_loss: 0.4004 - val_capsnet_loss: 0.3747 - val_decoder_loss: 0.0656 - val_capsnet_accuracy: 0.4565 - lr: 1.0000e-04\n",
            "Epoch 3/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3227 - capsnet_loss: 0.2959 - decoder_loss: 0.0682 - capsnet_accuracy: 0.5714\n",
            "Epoch 3: val_capsnet_accuracy improved from 0.45652 to 0.54348, saving model to ./result/weights.h5\n",
            "11/11 [==============================] - 7s 653ms/step - loss: 0.3227 - capsnet_loss: 0.2959 - decoder_loss: 0.0682 - capsnet_accuracy: 0.5714 - val_loss: 0.3026 - val_capsnet_loss: 0.2770 - val_decoder_loss: 0.0654 - val_capsnet_accuracy: 0.5435 - lr: 1.0000e-04\n",
            "Epoch 4/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2713 - capsnet_loss: 0.2445 - decoder_loss: 0.0683 - capsnet_accuracy: 0.5179\n",
            "Epoch 4: val_capsnet_accuracy did not improve from 0.54348\n",
            "11/11 [==============================] - 7s 629ms/step - loss: 0.2713 - capsnet_loss: 0.2445 - decoder_loss: 0.0683 - capsnet_accuracy: 0.5179 - val_loss: 0.2676 - val_capsnet_loss: 0.2421 - val_decoder_loss: 0.0650 - val_capsnet_accuracy: 0.5435 - lr: 1.0000e-04\n",
            "Epoch 5/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2584 - capsnet_loss: 0.2321 - decoder_loss: 0.0670 - capsnet_accuracy: 0.4911\n",
            "Epoch 5: val_capsnet_accuracy did not improve from 0.54348\n",
            "11/11 [==============================] - 7s 630ms/step - loss: 0.2584 - capsnet_loss: 0.2321 - decoder_loss: 0.0670 - capsnet_accuracy: 0.4911 - val_loss: 0.2494 - val_capsnet_loss: 0.2242 - val_decoder_loss: 0.0641 - val_capsnet_accuracy: 0.5435 - lr: 1.0000e-04\n",
            "Epoch 6/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2561 - capsnet_loss: 0.2301 - decoder_loss: 0.0664 - capsnet_accuracy: 0.5060\n",
            "Epoch 6: val_capsnet_accuracy did not improve from 0.54348\n",
            "11/11 [==============================] - 7s 646ms/step - loss: 0.2561 - capsnet_loss: 0.2301 - decoder_loss: 0.0664 - capsnet_accuracy: 0.5060 - val_loss: 0.2443 - val_capsnet_loss: 0.2198 - val_decoder_loss: 0.0627 - val_capsnet_accuracy: 0.4783 - lr: 1.0000e-04\n",
            "Epoch 7/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2300 - capsnet_loss: 0.2051 - decoder_loss: 0.0636 - capsnet_accuracy: 0.6280\n",
            "Epoch 7: val_capsnet_accuracy improved from 0.54348 to 0.59783, saving model to ./result/weights.h5\n",
            "11/11 [==============================] - 7s 666ms/step - loss: 0.2300 - capsnet_loss: 0.2051 - decoder_loss: 0.0636 - capsnet_accuracy: 0.6280 - val_loss: 0.2262 - val_capsnet_loss: 0.2027 - val_decoder_loss: 0.0600 - val_capsnet_accuracy: 0.5978 - lr: 1.0000e-04\n",
            "Epoch 8/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2214 - capsnet_loss: 0.1976 - decoder_loss: 0.0608 - capsnet_accuracy: 0.6607\n",
            "Epoch 8: val_capsnet_accuracy improved from 0.59783 to 0.71739, saving model to ./result/weights.h5\n",
            "11/11 [==============================] - 7s 650ms/step - loss: 0.2214 - capsnet_loss: 0.1976 - decoder_loss: 0.0608 - capsnet_accuracy: 0.6607 - val_loss: 0.2117 - val_capsnet_loss: 0.1902 - val_decoder_loss: 0.0548 - val_capsnet_accuracy: 0.7174 - lr: 1.0000e-04\n",
            "Epoch 9/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1911 - capsnet_loss: 0.1703 - decoder_loss: 0.0533 - capsnet_accuracy: 0.7917\n",
            "Epoch 9: val_capsnet_accuracy improved from 0.71739 to 0.73913, saving model to ./result/weights.h5\n",
            "11/11 [==============================] - 7s 657ms/step - loss: 0.1911 - capsnet_loss: 0.1703 - decoder_loss: 0.0533 - capsnet_accuracy: 0.7917 - val_loss: 0.2088 - val_capsnet_loss: 0.1904 - val_decoder_loss: 0.0470 - val_capsnet_accuracy: 0.7391 - lr: 1.0000e-04\n",
            "Epoch 10/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1915 - capsnet_loss: 0.1747 - decoder_loss: 0.0429 - capsnet_accuracy: 0.7054\n",
            "Epoch 10: val_capsnet_accuracy improved from 0.73913 to 0.75000, saving model to ./result/weights.h5\n",
            "11/11 [==============================] - 7s 665ms/step - loss: 0.1915 - capsnet_loss: 0.1747 - decoder_loss: 0.0429 - capsnet_accuracy: 0.7054 - val_loss: 0.1953 - val_capsnet_loss: 0.1818 - val_decoder_loss: 0.0344 - val_capsnet_accuracy: 0.7500 - lr: 1.0000e-04\n",
            "Epoch 11/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1701 - capsnet_loss: 0.1589 - decoder_loss: 0.0286 - capsnet_accuracy: 0.7411\n",
            "Epoch 11: val_capsnet_accuracy improved from 0.75000 to 0.77174, saving model to ./result/weights.h5\n",
            "11/11 [==============================] - 7s 647ms/step - loss: 0.1701 - capsnet_loss: 0.1589 - decoder_loss: 0.0286 - capsnet_accuracy: 0.7411 - val_loss: 0.1766 - val_capsnet_loss: 0.1687 - val_decoder_loss: 0.0202 - val_capsnet_accuracy: 0.7717 - lr: 1.0000e-04\n",
            "Epoch 12/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1514 - capsnet_loss: 0.1451 - decoder_loss: 0.0162 - capsnet_accuracy: 0.7946\n",
            "Epoch 12: val_capsnet_accuracy did not improve from 0.77174\n",
            "11/11 [==============================] - 7s 648ms/step - loss: 0.1514 - capsnet_loss: 0.1451 - decoder_loss: 0.0162 - capsnet_accuracy: 0.7946 - val_loss: 0.1671 - val_capsnet_loss: 0.1625 - val_decoder_loss: 0.0119 - val_capsnet_accuracy: 0.6957 - lr: 1.0000e-04\n",
            "Epoch 13/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1386 - capsnet_loss: 0.1341 - decoder_loss: 0.0115 - capsnet_accuracy: 0.8153\n",
            "Epoch 13: val_capsnet_accuracy improved from 0.77174 to 0.78261, saving model to ./result/weights.h5\n",
            "11/11 [==============================] - 7s 675ms/step - loss: 0.1386 - capsnet_loss: 0.1341 - decoder_loss: 0.0115 - capsnet_accuracy: 0.8153 - val_loss: 0.1593 - val_capsnet_loss: 0.1555 - val_decoder_loss: 0.0096 - val_capsnet_accuracy: 0.7826 - lr: 1.0000e-04\n",
            "Epoch 14/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1236 - capsnet_loss: 0.1195 - decoder_loss: 0.0103 - capsnet_accuracy: 0.8185\n",
            "Epoch 14: val_capsnet_accuracy did not improve from 0.78261\n",
            "11/11 [==============================] - 7s 669ms/step - loss: 0.1236 - capsnet_loss: 0.1195 - decoder_loss: 0.0103 - capsnet_accuracy: 0.8185 - val_loss: 0.1518 - val_capsnet_loss: 0.1480 - val_decoder_loss: 0.0096 - val_capsnet_accuracy: 0.7826 - lr: 1.0000e-04\n",
            "Epoch 15/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1196 - capsnet_loss: 0.1158 - decoder_loss: 0.0095 - capsnet_accuracy: 0.8333\n",
            "Epoch 15: val_capsnet_accuracy improved from 0.78261 to 0.79348, saving model to ./result/weights.h5\n",
            "11/11 [==============================] - 7s 647ms/step - loss: 0.1196 - capsnet_loss: 0.1158 - decoder_loss: 0.0095 - capsnet_accuracy: 0.8333 - val_loss: 0.1654 - val_capsnet_loss: 0.1615 - val_decoder_loss: 0.0100 - val_capsnet_accuracy: 0.7935 - lr: 1.0000e-04\n",
            "Best Validation Accuracy:0.79347825050354\n",
            "------------------------------Begin: test------------------------------\n",
            "Test acc: 0.7934782608695652\n",
            "Precision:0.8297872340425532\n",
            "recall:0.78\n",
            "f1:0.8041237113402062\n",
            "AUC:0.7947619047619048\n",
            "Sensitivity:0.78\n",
            "specificity:0.8095238095238095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "z6_kkr0YqB-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-i3c_UESE2P",
        "outputId": "b2c555ac-1fc2-42e8-f75d-edd075e3a9b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "set_seed()\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = get_rim_one_db_r2()\n",
        "\n",
        "    # define model\n",
        "model, eval_model = CapsNet(input_shape=x_train.shape[1:],\n",
        "                                                  n_class=2,\n",
        "                                                  routings=args.routings,\n",
        "                                                  batch_size=args.batch_size)\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(32, 64, 64, 3)]    0           []                               \n",
            "                                                                                                  \n",
            " conv1 (Conv2D)                 (32, 56, 56, 256)    62464       ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " primarycap_conv2d (Conv2D)     (32, 24, 24, 256)    5308672     ['conv1[0][0]']                  \n",
            "                                                                                                  \n",
            " primarycap_reshape (Reshape)   (32, 18432, 8)       0           ['primarycap_conv2d[0][0]']      \n",
            "                                                                                                  \n",
            " primarycap_squash (Lambda)     (32, 18432, 8)       0           ['primarycap_reshape[0][0]']     \n",
            "                                                                                                  \n",
            " digitcaps (CapsuleLayer)       (32, 2, 16)          4718592     ['primarycap_squash[0][0]']      \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 2)]          0           []                               \n",
            "                                                                                                  \n",
            " mask (Mask)                    (32, 32)             0           ['digitcaps[0][0]',              \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " capsnet (Length)               (32, 2)              0           ['digitcaps[0][0]']              \n",
            "                                                                                                  \n",
            " decoder (Sequential)           (None, 64, 64, 3)    13137408    ['mask[0][0]']                   \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,227,136\n",
            "Trainable params: 23,227,136\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJxjBAqnX-IQ",
        "outputId": "cd12382a-4019-4718-bf34-2c08a243c461"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "2fda2834-7c9e-4ed6-b322-0b9e86415201",
        "_uuid": "07bbb33aaa1c7ec875798359e300bbcdba374659",
        "id": "8NCOZGHlo8yM",
        "outputId": "c777ca26-ec43-4c6c-b7cb-c754e2a262c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#train(model=model, data=((x_train, y_train), (x_test[:60], y_test[:60])), \n",
        " #     epoch_size_frac = 0.5) # do 10% of an epoch (takes too long)\n",
        "trained_model = train(model=model, data=((x_train, y_train), (x_test, y_test)), args=args)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/11 [==============================] - ETA: 0s - loss: 0.4703 - capsnet_loss: 0.4434 - decoder_loss: 0.0687 - capsnet_accuracy: 0.5057 \n",
            "Epoch 1: val_capsnet_accuracy improved from -inf to 0.45652, saving model to ./result/weights-01.h5\n",
            "11/11 [==============================] - 174s 16s/step - loss: 0.4703 - capsnet_loss: 0.4434 - decoder_loss: 0.0687 - capsnet_accuracy: 0.5057 - val_loss: 0.4225 - val_capsnet_loss: 0.3966 - val_decoder_loss: 0.0659 - val_capsnet_accuracy: 0.4565 - lr: 1.0000e-04\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4027 - capsnet_loss: 0.3762 - decoder_loss: 0.0676 - capsnet_accuracy: 0.4167 \n",
            "Epoch 2: val_capsnet_accuracy did not improve from 0.45652\n",
            "11/11 [==============================] - 190s 18s/step - loss: 0.4027 - capsnet_loss: 0.3762 - decoder_loss: 0.0676 - capsnet_accuracy: 0.4167 - val_loss: 0.4050 - val_capsnet_loss: 0.3793 - val_decoder_loss: 0.0656 - val_capsnet_accuracy: 0.3804 - lr: 1.0000e-04\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3520 - capsnet_loss: 0.3251 - decoder_loss: 0.0686 - capsnet_accuracy: 0.5625 \n",
            "Epoch 3: val_capsnet_accuracy improved from 0.45652 to 0.54348, saving model to ./result/weights-03.h5\n",
            "11/11 [==============================] - 174s 16s/step - loss: 0.3520 - capsnet_loss: 0.3251 - decoder_loss: 0.0686 - capsnet_accuracy: 0.5625 - val_loss: 0.3266 - val_capsnet_loss: 0.3009 - val_decoder_loss: 0.0654 - val_capsnet_accuracy: 0.5435 - lr: 1.0000e-04\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2990 - capsnet_loss: 0.2727 - decoder_loss: 0.0670 - capsnet_accuracy: 0.5179 \n",
            "Epoch 4: val_capsnet_accuracy did not improve from 0.54348\n",
            "11/11 [==============================] - 183s 17s/step - loss: 0.2990 - capsnet_loss: 0.2727 - decoder_loss: 0.0670 - capsnet_accuracy: 0.5179 - val_loss: 0.2690 - val_capsnet_loss: 0.2436 - val_decoder_loss: 0.0648 - val_capsnet_accuracy: 0.4565 - lr: 1.0000e-04\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2757 - capsnet_loss: 0.2490 - decoder_loss: 0.0681 - capsnet_accuracy: 0.4643 \n",
            "Epoch 5: val_capsnet_accuracy did not improve from 0.54348\n",
            "11/11 [==============================] - 166s 15s/step - loss: 0.2757 - capsnet_loss: 0.2490 - decoder_loss: 0.0681 - capsnet_accuracy: 0.4643 - val_loss: 0.2595 - val_capsnet_loss: 0.2345 - val_decoder_loss: 0.0638 - val_capsnet_accuracy: 0.5435 - lr: 1.0000e-04\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2540 - capsnet_loss: 0.2286 - decoder_loss: 0.0648 - capsnet_accuracy: 0.5565 \n",
            "Epoch 6: val_capsnet_accuracy improved from 0.54348 to 0.55435, saving model to ./result/weights-06.h5\n",
            "11/11 [==============================] - 200s 18s/step - loss: 0.2540 - capsnet_loss: 0.2286 - decoder_loss: 0.0648 - capsnet_accuracy: 0.5565 - val_loss: 0.2466 - val_capsnet_loss: 0.2222 - val_decoder_loss: 0.0621 - val_capsnet_accuracy: 0.5543 - lr: 1.0000e-04\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2518 - capsnet_loss: 0.2268 - decoder_loss: 0.0639 - capsnet_accuracy: 0.5268 \n",
            "Epoch 7: val_capsnet_accuracy did not improve from 0.55435\n",
            "11/11 [==============================] - 190s 17s/step - loss: 0.2518 - capsnet_loss: 0.2268 - decoder_loss: 0.0639 - capsnet_accuracy: 0.5268 - val_loss: 0.2458 - val_capsnet_loss: 0.2227 - val_decoder_loss: 0.0590 - val_capsnet_accuracy: 0.5435 - lr: 1.0000e-04\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2366 - capsnet_loss: 0.2136 - decoder_loss: 0.0588 - capsnet_accuracy: 0.6250 \n",
            "Epoch 8: val_capsnet_accuracy improved from 0.55435 to 0.65217, saving model to ./result/weights-08.h5\n",
            "11/11 [==============================] - 164s 15s/step - loss: 0.2366 - capsnet_loss: 0.2136 - decoder_loss: 0.0588 - capsnet_accuracy: 0.6250 - val_loss: 0.2329 - val_capsnet_loss: 0.2119 - val_decoder_loss: 0.0536 - val_capsnet_accuracy: 0.6522 - lr: 1.0000e-04\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2210 - capsnet_loss: 0.2006 - decoder_loss: 0.0520 - capsnet_accuracy: 0.6488 \n",
            "Epoch 9: val_capsnet_accuracy improved from 0.65217 to 0.73913, saving model to ./result/weights-09.h5\n",
            "11/11 [==============================] - 161s 15s/step - loss: 0.2210 - capsnet_loss: 0.2006 - decoder_loss: 0.0520 - capsnet_accuracy: 0.6488 - val_loss: 0.2081 - val_capsnet_loss: 0.1908 - val_decoder_loss: 0.0441 - val_capsnet_accuracy: 0.7391 - lr: 1.0000e-04\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1962 - capsnet_loss: 0.1805 - decoder_loss: 0.0399 - capsnet_accuracy: 0.7054 \n",
            "Epoch 10: val_capsnet_accuracy did not improve from 0.73913\n",
            "11/11 [==============================] - 159s 15s/step - loss: 0.1962 - capsnet_loss: 0.1805 - decoder_loss: 0.0399 - capsnet_accuracy: 0.7054 - val_loss: 0.2034 - val_capsnet_loss: 0.1915 - val_decoder_loss: 0.0303 - val_capsnet_accuracy: 0.6630 - lr: 1.0000e-04\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1943 - capsnet_loss: 0.1838 - decoder_loss: 0.0270 - capsnet_accuracy: 0.6935 \n",
            "Epoch 11: val_capsnet_accuracy improved from 0.73913 to 0.75000, saving model to ./result/weights-11.h5\n",
            "11/11 [==============================] - 203s 19s/step - loss: 0.1943 - capsnet_loss: 0.1838 - decoder_loss: 0.0270 - capsnet_accuracy: 0.6935 - val_loss: 0.1855 - val_capsnet_loss: 0.1781 - val_decoder_loss: 0.0189 - val_capsnet_accuracy: 0.7500 - lr: 1.0000e-04\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1831 - capsnet_loss: 0.1768 - decoder_loss: 0.0161 - capsnet_accuracy: 0.7500 \n",
            "Epoch 12: val_capsnet_accuracy did not improve from 0.75000\n",
            "11/11 [==============================] - 169s 15s/step - loss: 0.1831 - capsnet_loss: 0.1768 - decoder_loss: 0.0161 - capsnet_accuracy: 0.7500 - val_loss: 0.1803 - val_capsnet_loss: 0.1753 - val_decoder_loss: 0.0126 - val_capsnet_accuracy: 0.6413 - lr: 1.0000e-04\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1793 - capsnet_loss: 0.1746 - decoder_loss: 0.0120 - capsnet_accuracy: 0.7159 \n",
            "Epoch 13: val_capsnet_accuracy did not improve from 0.75000\n",
            "11/11 [==============================] - 153s 14s/step - loss: 0.1793 - capsnet_loss: 0.1746 - decoder_loss: 0.0120 - capsnet_accuracy: 0.7159 - val_loss: 0.1632 - val_capsnet_loss: 0.1593 - val_decoder_loss: 0.0099 - val_capsnet_accuracy: 0.7174 - lr: 1.0000e-04\n",
            "Epoch 14/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1551 - capsnet_loss: 0.1509 - decoder_loss: 0.0106 - capsnet_accuracy: 0.7560 \n",
            "Epoch 14: val_capsnet_accuracy improved from 0.75000 to 0.78261, saving model to ./result/weights-14.h5\n",
            "11/11 [==============================] - 145s 14s/step - loss: 0.1551 - capsnet_loss: 0.1509 - decoder_loss: 0.0106 - capsnet_accuracy: 0.7560 - val_loss: 0.1629 - val_capsnet_loss: 0.1592 - val_decoder_loss: 0.0093 - val_capsnet_accuracy: 0.7826 - lr: 1.0000e-04\n",
            "Epoch 15/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1522 - capsnet_loss: 0.1479 - decoder_loss: 0.0109 - capsnet_accuracy: 0.7560 \n",
            "Epoch 15: val_capsnet_accuracy did not improve from 0.78261\n",
            "11/11 [==============================] - 159s 15s/step - loss: 0.1522 - capsnet_loss: 0.1479 - decoder_loss: 0.0109 - capsnet_accuracy: 0.7560 - val_loss: 0.1532 - val_capsnet_loss: 0.1496 - val_decoder_loss: 0.0093 - val_capsnet_accuracy: 0.7500 - lr: 1.0000e-04\n",
            "Epoch 16/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1503 - capsnet_loss: 0.1463 - decoder_loss: 0.0100 - capsnet_accuracy: 0.7560 \n",
            "Epoch 16: val_capsnet_accuracy did not improve from 0.78261\n",
            "11/11 [==============================] - 146s 13s/step - loss: 0.1503 - capsnet_loss: 0.1463 - decoder_loss: 0.0100 - capsnet_accuracy: 0.7560 - val_loss: 0.1623 - val_capsnet_loss: 0.1587 - val_decoder_loss: 0.0092 - val_capsnet_accuracy: 0.6848 - lr: 1.0000e-04\n",
            "Epoch 17/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1554 - capsnet_loss: 0.1513 - decoder_loss: 0.0105 - capsnet_accuracy: 0.7470 \n",
            "Epoch 17: val_capsnet_accuracy did not improve from 0.78261\n",
            "11/11 [==============================] - 146s 13s/step - loss: 0.1554 - capsnet_loss: 0.1513 - decoder_loss: 0.0105 - capsnet_accuracy: 0.7470 - val_loss: 0.1397 - val_capsnet_loss: 0.1362 - val_decoder_loss: 0.0088 - val_capsnet_accuracy: 0.7500 - lr: 1.0000e-04\n",
            "Epoch 18/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1560 - capsnet_loss: 0.1520 - decoder_loss: 0.0103 - capsnet_accuracy: 0.7738 \n",
            "Epoch 18: val_capsnet_accuracy did not improve from 0.78261\n",
            "11/11 [==============================] - 146s 13s/step - loss: 0.1560 - capsnet_loss: 0.1520 - decoder_loss: 0.0103 - capsnet_accuracy: 0.7738 - val_loss: 0.1593 - val_capsnet_loss: 0.1558 - val_decoder_loss: 0.0088 - val_capsnet_accuracy: 0.7500 - lr: 1.0000e-04\n",
            "Epoch 19/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1428 - capsnet_loss: 0.1386 - decoder_loss: 0.0107 - capsnet_accuracy: 0.8036 \n",
            "Epoch 19: val_capsnet_accuracy did not improve from 0.78261\n",
            "11/11 [==============================] - 146s 13s/step - loss: 0.1428 - capsnet_loss: 0.1386 - decoder_loss: 0.0107 - capsnet_accuracy: 0.8036 - val_loss: 0.1592 - val_capsnet_loss: 0.1557 - val_decoder_loss: 0.0091 - val_capsnet_accuracy: 0.6630 - lr: 1.0000e-04\n",
            "Epoch 20/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1420 - capsnet_loss: 0.1381 - decoder_loss: 0.0098 - capsnet_accuracy: 0.7887 \n",
            "Epoch 20: val_capsnet_accuracy did not improve from 0.78261\n",
            "11/11 [==============================] - 145s 13s/step - loss: 0.1420 - capsnet_loss: 0.1381 - decoder_loss: 0.0098 - capsnet_accuracy: 0.7887 - val_loss: 0.1812 - val_capsnet_loss: 0.1773 - val_decoder_loss: 0.0099 - val_capsnet_accuracy: 0.6630 - lr: 1.0000e-04\n",
            "Trained model saved to './result/trained_model.h5'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2706080d-2d50-4876-bd4d-c0f3c4ce87b5",
        "_uuid": "9afcde53f3cf3ba3eeeed1f083241874b4cd84e2",
        "id": "lMRMqKA9o8yS"
      },
      "source": [
        "# Show the results on the hold-out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvn3MxxUAzrW",
        "outputId": "f1b01a33-be51-4487-c043-b1a052da9af2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(92, 64, 64, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model.load_weights('./result/weights-14.h5')"
      ],
      "metadata": {
        "id": "MaQPw7FyCZTb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "56a6fe58-fe97-45c8-95da-223297842f79",
        "_uuid": "25ac7feb2d111b82e755169288ffd47ebc4d196a",
        "id": "PL8it8clo8yT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0df24ea0-f0e2-4150-9a10-422bf5cd6acc"
      },
      "source": [
        "\n",
        "y_pred,_= trained_model.predict((x_test,y_test), batch_size=46)\n",
        "print('-' * 30 + 'Begin: test' + '-' * 30)\n",
        "print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1)) / y_test.shape[0])\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------Begin: test------------------------------\n",
            "Test acc: 0.6630434782608695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiBpPdkmY500",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5479bba-38d6-4a73-bf9e-cb9e945287a5"
      },
      "source": [
        "y_pred,_= trained_model.predict((x_test,y_test), batch_size=x_test.shape[0])\n",
        "print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1)) / y_test.shape[0])  \n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test acc: 0.782608695652174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_pred = np.argmax(y_pred, axis=1) \n",
        "result_actual = np.argmax(y_test, axis=1) \n",
        "precision = precision_score(result_actual, result_pred)\n",
        "recall = recall_score(result_actual, result_pred)\n",
        "f1 = f1_score(result_actual, result_pred)\n",
        "tn, fp, fn, tp = confusion_matrix(result_actual, result_pred).ravel()\n",
        "fpr, tpr, thresholds = metrics.roc_curve(result_actual, result_pred)\n",
        "auc = metrics.auc(fpr, tpr)\n",
        "  \n",
        "print(f\"Precision:{precision}\")\n",
        "print(f\"recall:{recall}\")\n",
        "print(f\"f1:{f1}\")\n",
        "print(f\"AUC:{auc}\")\n",
        "print(f\"Sensitivity:{tp/(tp+fn)}\")\n",
        "print(f\"specificity:{tn/(tn+fp)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV3blkNqh4-P",
        "outputId": "a9614d3d-bd3e-49fb-f6f1-cfbf82681ce7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision:0.8260869565217391\n",
            "recall:0.76\n",
            "f1:0.7916666666666667\n",
            "AUC:0.7847619047619048\n",
            "Sensitivity:0.76\n",
            "specificity:0.8095238095238095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "tn, fp, fn, tp = confusion_matrix(result_actual, result_pred).ravel()\n",
        "print(f\"Sensitivity:{tp/(tp+fn)}\")\n",
        "print(f\"specificity:{tn/(tn+fp)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVYRbZ9iIL-",
        "outputId": "a7c3ede4-110f-4084-ee56-5b21e96e1015"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity:0.76\n",
            "specificity:0.8095238095238095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, thresholds = metrics.roc_curve(result_actual, result_pred)"
      ],
      "metadata": {
        "id": "ZezAT2KYlrZz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.auc(fpr, tpr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5NqTR4Jn5r7",
        "outputId": "e6644851-ea27-4394-cb50-340fb5cb0a80"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7847619047619048"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}